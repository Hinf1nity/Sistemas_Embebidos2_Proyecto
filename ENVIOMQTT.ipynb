{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import paho.mqtt.client as mqtt\n",
    "move = 0\n",
    "client = mqtt.Client()\n",
    "# Cargar el modelo desde el archivo H5\n",
    "model = load_model('modelo1_5classes.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_connect(client, userdata, flags, rc):\n",
    "    print(\"Connected with result code \"+str(rc))\n",
    "    client.subscribe(\"proyecto/estado\")\n",
    "\n",
    "def on_message(client, userdata, msg):\n",
    "    print(msg.topic+\" \"+str(msg.payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.on_connect = on_connect\n",
    "client.on_message = on_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step\n",
      "['AUMENTAR']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#-------------------------CODIGO PARA GRABACION DEL VIDEO----------------------------------\n",
    "\n",
    "def grabar_video():\n",
    "    # Establecer las dimensiones deseadas del video\n",
    "    width = 640\n",
    "    height = 396\n",
    "\n",
    "    # Inicializar la captura de video desde la webcam\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Establecer el tamaño del cuadro de captura\n",
    "    video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "    video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "    # Obtener la información de tamaño del video\n",
    "    width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = 30  # FPS (cuadros por segundo) del video\n",
    "\n",
    "    # Crear el objeto de salida de video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_out = cv2.VideoWriter('video_salida.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "    # Grabar video durante 5 segundos\n",
    "    tiempo_inicio = cv2.getTickCount()  # Obtener el tiempo actual en ticks\n",
    "    duracion_grabacion = 3  # Duración de la grabación en segundos\n",
    "\n",
    "    while True:\n",
    "        # Capturar un cuadro de video\n",
    "        ret, frame = video_capture.read()\n",
    "\n",
    "        if ret:\n",
    "            # Escribir el cuadro en el archivo de salida\n",
    "            video_out.write(frame)\n",
    "\n",
    "            # Calcular el tiempo transcurrido\n",
    "            tiempo_actual = cv2.getTickCount()\n",
    "            tiempo_transcurrido = (tiempo_actual - tiempo_inicio) / cv2.getTickFrequency()\n",
    "\n",
    "            # Detener la grabación después de 5 segundos\n",
    "            if tiempo_transcurrido >= duracion_grabacion:\n",
    "                break\n",
    "\n",
    "        # Mostrar el cuadro de video en una ventana\n",
    "        cv2.imshow('Video', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Liberar los recursos\n",
    "    video_capture.release()\n",
    "    video_out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Llamar a la función para grabar el video\n",
    "grabar_video()\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "#-----------------------------CODIGO PARA SEGMENTACION DE PIEL ----------------------\n",
    "def equaliseCLAHEOpenCV(image):\n",
    "    image_base = np.array(image, copy=True)\n",
    "\n",
    "  # Convert if it's an RGB image\n",
    "    if len(image_base.shape) > 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)[:,:,0]\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(9,9))\n",
    "    cl1 = clahe.apply(image)\n",
    "\n",
    "  # Inverse conversion if it was an RGB image\n",
    "    if len(image_base.shape) > 2:\n",
    "        image_aux = cv2.cvtColor(image_base, cv2.COLOR_RGB2YUV)\n",
    "        image_aux[:,:,0] = cl1\n",
    "        result = cv2.cvtColor(image_aux,cv2.COLOR_YUV2RGB)\n",
    "  \n",
    "    return result\n",
    "\n",
    "\n",
    "def segmentar_piel(frame):\n",
    "    # Aplicar mejoramiento de contraste\n",
    "    frame = equaliseCLAHEOpenCV(frame)\n",
    "    ycbcr = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "    # Definir los rangos de color para la piel en el espacio de color YCbCr\n",
    "    bajo = np.array([0, 133, 50], dtype=np.uint8)\n",
    "    alto = np.array([255, 173, 200], dtype=np.uint8)\n",
    "\n",
    "    # Crear la máscara de piel en el espacio de color YCbCr\n",
    "    mascara_piel = cv2.inRange(ycbcr, bajo, alto)\n",
    "    #IRMPRIIR ALGO SI MARCARA PIEL ESTA VACIO O NO\n",
    "    if mascara_piel.sum() == 0:\n",
    "        print(\"No se detecto piel\")\n",
    "\n",
    "    # Aplicar la máscara a la imagen original\n",
    "    frame_sinfondo = frame.copy()\n",
    "    frame_sinfondo[mascara_piel == 0] = [0, 0, 0]\n",
    "\n",
    "    return frame_sinfondo\n",
    "\n",
    "\n",
    "def preprocess_video(input_video_path, output_video_path, target_resolution=(704, 396), target_frames=100):\n",
    "    video = cv2.VideoCapture(input_video_path)\n",
    "    frames_total = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    sampling_interval = max(frames_total // target_frames, 1)\n",
    "    selected_frames = []\n",
    "\n",
    "    for frame_idx in range(frames_total):\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        if frame_idx % sampling_interval == 0:\n",
    "            frame = cv2.resize(frame, target_resolution)\n",
    "            frame = segmentar_piel(frame)\n",
    "            selected_frames.append(frame)\n",
    "\n",
    "    video.release()\n",
    "    selected_frames = np.array(selected_frames)\n",
    "\n",
    "    if selected_frames.shape[0] < target_frames:\n",
    "        selected_frames = np.pad(selected_frames, [(0, target_frames - selected_frames.shape[0]), (0, 0), (0, 0), (0, 0)], mode='wrap')\n",
    "    else:\n",
    "        selected_frames = selected_frames[:target_frames]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(selected_frames)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter(output_video_path, fourcc, fps, target_resolution)\n",
    "\n",
    "    for frame in dataset:\n",
    "        frame = frame.numpy().astype(np.uint8)\n",
    "        output_video.write(frame)\n",
    "\n",
    "    output_video.release()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "#Se pone como primer parametro el video de entrada\n",
    "#Se pone como segundo parametro el nombre del video de salida\n",
    "preprocess_video('video_salida.mp4', 'video_piel.mp4')\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------CODIGO PARA DETECCION DE MANOS ------------------------\n",
    "# Cargar el módulo de Mediapipe para detección de manos\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Cargar el video\n",
    "video_path = \"video_piel.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Configurar la salida de la imagen resumen\n",
    "summary_output_path = \"salida.jpg\"\n",
    "\n",
    "# Configurar los puntos clave que se desean mostrar\n",
    "desired_keypoints = [5, 9, 13]  # Índices de los puntos clave de interés\n",
    "\n",
    "\n",
    "# Configurar Mediapipe para la detección de manos\n",
    "with mp_hands.Hands(static_image_mode=True, max_num_hands=2,\n",
    "                    min_detection_confidence=0.05, min_tracking_confidence=0.03) as hands:\n",
    "    frame_count = 0\n",
    "    summary_image = np.zeros((396, 640, 3))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        # Leer el siguiente frame\n",
    "        success, image = cap.read()\n",
    "        #redimensionar la imagen\n",
    "        image = cv2.resize(image, (640, 396))\n",
    "        #image=cv2.re\n",
    "        black_image = np.zeros_like(image)\n",
    "        colors = [(0, 0, 255), (0, 255, 0), (255, 0, 0)] \n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "        # Detección de manos\n",
    "        results_hands = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "        if results_hands.multi_hand_landmarks:\n",
    "\n",
    "            for hand_landmarks in results_hands.multi_hand_landmarks:\n",
    "                for idx, landmark in enumerate(hand_landmarks.landmark):\n",
    "                    if idx in desired_keypoints:\n",
    "                        x = int(landmark.x * image.shape[1])\n",
    "                        y = int(landmark.y * image.shape[0])\n",
    "\n",
    "                        color = colors[desired_keypoints.index(idx)]  # Obtener el color correspondiente al punto clave\n",
    "                        cv2.circle(black_image, (x, y), 2, color, -1)\n",
    "\n",
    "\n",
    "            summary_image += black_image.astype(np.float32)\n",
    "\n",
    "        frame_count += 1   \n",
    "        if frame_count == 100:\n",
    "            break\n",
    "    \n",
    "    summary_image = np.clip(summary_image, 0, 255).astype(np.uint8)\n",
    "    cv2.imwrite(summary_output_path, summary_image)\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "#-----------------------------CODIGO PARA CLASIFICACION DE GESTO -----------------------\n",
    "\n",
    "\n",
    "# Preprocesar la imagen de entrada\n",
    "img = load_img('salida.jpg')\n",
    "img = tf.image.resize(img, (640, 396))\n",
    "test = np.array([img])\n",
    "\n",
    "# Realizar la predicción utilizando el modelo cargado\n",
    "predictions = model.predict(test)\n",
    "\n",
    "# Interpretar las predicciones\n",
    "#labels = ['CUANDO', 'DERECHA', 'DISMINUIR', 'IZQUIERDA', 'SI']\n",
    "labels = ['AVANZAR', 'DERECHA', 'DISMINUIR', 'IZQUIERDA', 'AUMENTAR']\n",
    "predicted_labels = [labels[np.argmax(prediction)] for prediction in predictions]\n",
    "\n",
    "print(predicted_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.connect(\"mqtt-dashboard.com\", 1883, 60)\n",
    "client.loop_start()\n",
    "if predicted_labels[0] == 'AVANZAR':\n",
    "    print(\"x\")\n",
    "    if move == 0:\n",
    "        print(\"y\")\n",
    "        client.publish(\"proyecto/estado\", \"l/1\")\n",
    "        move=1\n",
    "    elif move == 1:\n",
    "        print(\"z\")\n",
    "        client.publish(\"proyecto/estado\", \"l/4\")\n",
    "        move=0\n",
    "elif predicted_labels[0] == 'DERECHA':\n",
    "    client.publish(\"proyecto/estado\", \"l/2\")\n",
    "elif predicted_labels[0] == 'IZQUIERDA':\n",
    "    client.publish(\"proyecto/estado\", \"l/3\")\n",
    "elif predicted_labels[0] == 'AUMENTAR':\n",
    "    client.publish(\"proyecto/estado\", \"l/5\")\n",
    "elif predicted_labels[0] == 'DISMINUIR':\n",
    "    client.publish(\"proyecto/estado\", \"l/6\")\n",
    "client.loop_stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
